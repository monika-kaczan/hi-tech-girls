{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a0086a2",
   "metadata": {},
   "source": [
    "# Hi-Tech Girls Project by Monika Kaczan\n",
    "\n",
    "The main goal of this projects was to explore relationships between weather and functioning of BTS stations across Poland. I was given the data regarding weather in weather stations, as well as data regarding location and function of BTS stations.\n",
    "\n",
    "To explore relationships between the weather and functioning of BTS stations, I took the following steps:\n",
    "\n",
    "1. Data preprocessing\n",
    "   - Loading and transforming the data regarding the weather\n",
    "   - Completing the information about weather stations\n",
    "   - Checking the data correctness\n",
    "   - Aggregating the data about weather stations\n",
    "   - Handling missing values\n",
    "   - Loading the data on BTS stations\n",
    "   - Assigning BTS stations data on the weather\n",
    "   - Adding the data on BTS malfunctioning\n",
    "2. Data analysis\n",
    "   - Preliminary analysis for the weather stations data\n",
    "   - Preliminary analysis for the BTS stations data\n",
    "   - Comparing Orange and other providers\n",
    "   - Visualization of stations on the map\n",
    "   - Identification of stations that are exposed to the most extreme weather changes\n",
    "   - Assesing the impact of weather on BTS malfunctioning\n",
    "3. Training machine learning models\n",
    "   - Clustering: \n",
    "     - K-Means\n",
    "   - Classification model predicting a high percentage of anomaly occurences \n",
    "     - Logit\n",
    "     - Ridge Classifier\n",
    "     - XGBoost\n",
    "     - K nearest neighbours\n",
    "4. Conclusions\n",
    "   - Further project development\n",
    "   - Applying conclusions drawn from the data in practice\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Throught the notebook I am using the following variables which are further explained when neccessary (units assumed from the data):\n",
    "- A - air temperature (°C)\n",
    "- B - ground temperature (°C)\n",
    "- C - average wind speed (m/s)\n",
    "- D - daily total precipitation (mm)\n",
    "- E - relative air humidity (%)\n",
    "- anomalies - according to the definition provided I am refering to the percentage of anomalies in functioning of a BTS station throught the month. \n",
    "\n",
    "Throught the analysis I have used several additional files, for example Poland shapefile for vizualization or files with ML models that took long time to calculate.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c36af57",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acc8b9e",
   "metadata": {},
   "source": [
    "### Loading and transforming the data regarding weather"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0699a242",
   "metadata": {},
   "source": [
    "Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8222f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12e2b468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Choosing proper data files\n",
    "\n",
    "path = os. getcwd() + '/data'\n",
    "csv_files = glob.glob(path + \"./[A-E]_*\")\n",
    "df_list = (pd.read_csv(file, encoding = \"ISO-8859-2\", sep = \";\", decimal = ',', low_memory = False) for file in csv_files)\n",
    "\n",
    "# Combining information from all the files into one dataframe\n",
    "\n",
    "df_raw = pd.concat(df_list, ignore_index = True)\n",
    "df_master = df_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044691a5",
   "metadata": {},
   "source": [
    "Exploring the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0492b1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the data and change columns names\n",
    "\n",
    "df_master.info()\n",
    "display(df_master)\n",
    "df_master.columns = [\"id\", \"coef\", \"date\", \"value\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37225240",
   "metadata": {},
   "source": [
    "Handling missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4695ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the NAs\n",
    "\n",
    "df_master.isna().sum()\n",
    "# We see that we have lot's of NAs\n",
    "\n",
    "df_master.isnull().any(axis=1).sum()\n",
    "# However they are just empty rows so we can delete them\n",
    "\n",
    "df_master = df_master.dropna(how = 'all').reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3878ab",
   "metadata": {},
   "source": [
    "Handling duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc076ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the data for duplicates\n",
    "\n",
    "# Checking duplictes by rows\n",
    "df_master[df_master.duplicated()]\n",
    "\n",
    "# Deleting duplicates\n",
    "df_master = df_master.drop_duplicates()\n",
    "\n",
    "\n",
    "# Checking if we have rows with the same ID, Date and Coef but different\n",
    "\n",
    "df_master[df_master.duplicated(subset = ['id','coef', 'date'])]\n",
    "# We have observations with the same ID, Coef and Date, but different Value. \n",
    "# I assume the last (by row number) Value is correct, for example somebody made a mistake and tried to overrite it. \n",
    "\n",
    "# Deleting those duplicates\n",
    "df_master = df_master.drop_duplicates(subset = ['id','coef', 'date'], keep = 'last')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73fa4bc",
   "metadata": {},
   "source": [
    "Changing data format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a61b73fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_master = df_master.astype({'id':'int', 'coef': 'str'})\n",
    "df_master['date'] = pd.to_datetime(df_master['date'], format = \"%d.%m.%Y %H:%M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36d8d0b",
   "metadata": {},
   "source": [
    "Unstacking - we have all parameters in the same column, so we can unstack them and move them to different columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3c28060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we have all parameters in the same column, we can unstack them and move them to different columns.\n",
    "\n",
    "# First, let's check if parameters are written correctly.\n",
    "\n",
    "df_master['coef'].unique()\n",
    "# We see that we have Coef 'B' and 'B '. I assume that it is a mistake so I merge them.\n",
    "\n",
    "df_master['coef'] = df_master['coef'].replace('B ', 'B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f4c2b01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Unstacking\n",
    "\n",
    "df_unstacked = df_master.pivot_table(index = ['id', 'date'], columns = ['coef'], values = 'value').reset_index()\n",
    "df_unstacked\n",
    "\n",
    "# Checking if everything went okay and we have the same number of observations as before\n",
    "\n",
    "# df_master['Coef'].value_counts(dropna = True).sort_values()\n",
    "# df_unstacked.count().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73b02d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check NAs once again\n",
    "\n",
    "(df_unstacked.isna().sum(axis=1) > 0).sum()/len(df_unstacked) #We see that over 96% of rows contains at least one missing value - we cannot simply delete them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e37c03c",
   "metadata": {},
   "source": [
    "### Completing the information about weather stations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca198eeb",
   "metadata": {},
   "source": [
    "Connecting previously loaded data with data from kody_stacji.csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d487510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data file\n",
    "\n",
    "path_codes = os. getcwd() + '/data/kody_stacji.csv'\n",
    "df_codes = pd.read_csv(path_codes, encoding = \"ISO-8859-2\", sep = \";\", decimal = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99b5daf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look and the data and change names and formats\n",
    "\n",
    "df_codes.info()\n",
    "display(df_codes)\n",
    "\n",
    "df_codes.columns = [\"id\", \"name\", \"lat\", \"long\"]\n",
    "df_codes = df_codes.astype({'id':'int', 'name': 'str', 'lat': 'str', 'long': 'str'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc583eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the data about the weather and about the weather stations\n",
    "\n",
    "df_ws_merged = pd.merge(df_unstacked, df_codes, on ='id', how ='left') \n",
    "\n",
    "# Checking if it was correct\n",
    "\n",
    "# df_merged['Lat'].isnull().values.any()\n",
    "# df_merged['Long'].isnull().values.any()\n",
    "# df_merged['Name'].isnull().values.any()\n",
    "\n",
    "# No nulls so the data was merged okay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "235d9e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's also add data columns\n",
    "\n",
    "df_ws_merged['year'] = df_ws_merged['date'].dt.year \n",
    "df_ws_merged['month'] = df_ws_merged['date'].dt.month\n",
    "df_ws_merged['day'] = df_ws_merged['date'].dt.day\n",
    "\n",
    "df_ws_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c27002",
   "metadata": {},
   "source": [
    "### Checking the data correctness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e9c653",
   "metadata": {},
   "source": [
    "Before proceding to data aggregation and further analysis, I wanted to check the data correctness. To do that, I have taken a look at the basic statistics for the weather variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80f1207e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "df_ws_merged.drop(['id', 'day', 'month', 'year'], axis = 'columns').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1c2dda",
   "metadata": {},
   "source": [
    "Based on the table above, we can identify the following problems with the data:\n",
    "    \n",
    "- Variable C - average wind speed: we have very large maximum wind speeds. I assume that the unit for variable C are m/s (based on real data for Poland), so the wind speed of 1000 m/s is unreal and it is likely a incorrect observation.\n",
    "- Variable E: relative air humidity: according to the definition, relative air humidity cannot be lower than 0 (source: https://en.wikipedia.org/wiki/Humidity#Relative_humidity, https://www.chicagotribune.com/news/ct-xpm-2011-12-16-ct-wea-1216-asktom-20111216-story.html). \n",
    "\n",
    "The rest of the variables seem reasonable.\n",
    "\n",
    "Let's first tackle problems with variable C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bbb6fbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the statistics, I assume that the problem with high values is in few outliers.\n",
    "# Let's delete all observations with C value higher than 100 m/s which is approximately the highest wind speed ever recorded in Poland.\n",
    "\n",
    "df_ws_merged.C = np.where(df_ws_merged.C > 100, np.NaN, df_ws_merged.C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a560845f",
   "metadata": {},
   "source": [
    "Now let's correct variable E."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c97cb280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I assume that E values below 0 are incorrect. I trasform them into values above 0.\n",
    "# I delete values equal to 0.\n",
    "\n",
    "df_ws_merged.E = np.where(df_ws_merged.E < 0, df_ws_merged.E*-1, df_ws_merged.E)\n",
    "df_ws_merged.E = np.where(df_ws_merged.E == 0, np.NaN, df_ws_merged.E)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8993a2da",
   "metadata": {},
   "source": [
    "### Aggregating the data about weather stations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ab58d4",
   "metadata": {},
   "source": [
    "In the original file the data was provided at the level of hours. However, we have lot's of missing data and analyzing data at such a small level is not sensible. \n",
    "\n",
    "In this analysis I decided to aggregate the data at the month level, as the other data about BTS stations malfunctions is also aggregated at this level. Moreover, as not only the weather itself, but also its variability may, in my opinion, influence the functioning of BTS stations, I took into account both mean and standard deviation of weather variables across the month.\n",
    "\n",
    "I have chosen mean as a general indicator of this weather parameter throught the month. I also took standard deviations as a measure of variability of that parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e49fca38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregating and calculating monthly means of weather variables\n",
    "\n",
    "df_month_mean = df_ws_merged.loc[:, df_ws_merged.columns != 'day'].groupby(['id', 'name', 'lat', 'long', 'year', 'month']).mean(numeric_only = True)\n",
    "df_month_mean.reset_index(inplace = True)\n",
    "\n",
    "\n",
    "# Aggregating and calculating monthly standard deviations of weather variables\n",
    "\n",
    "df_month_std = df_ws_merged.loc[:, df_ws_merged.columns != 'Day'].groupby(['id', 'name', 'lat', 'long', 'year', 'month']).std(numeric_only = True)\n",
    "df_month_std.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "952ca5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's merge both datasets\n",
    "\n",
    "df_ws = pd.merge(df_month_mean, df_month_std, on = ['id', 'name', 'lat', 'long', 'year', 'month'], how = 'left', suffixes = ('_mean', '_std'))\n",
    "display(df_ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d15e0f8",
   "metadata": {},
   "source": [
    "To make the analysis more clear, I would like to focus only on one year. In the data we have two years: 2017 and 2022. \n",
    "\n",
    "Let's check if both years are similiar in terms of mean values of monthly weather variables using ANOVA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "61554d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "years = [2017, 2022]\n",
    "\n",
    "for variable in df_ws.columns[6:11]:  # Get the parameter columns\n",
    "    data = [df_ws[df_ws['year'] == year][variable].dropna() for year in years]\n",
    "    f_statistic, p_value = stats.f_oneway(*data)\n",
    "    print(variable, p_value) \n",
    "    \n",
    "# Based on p-value we cannot reject the null hypothesis that the corresponding variable means for each year are the same. \n",
    "# It means that we cannot see statistically significant differences in monthly mean values of parameters between the years."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63eaebe1",
   "metadata": {},
   "source": [
    "As both years seem similiar, I choose the most recent year of 2022 and the rest of my analysis will be based only on that data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b55d894b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ws2022 = df_ws[df_ws['year'] == 2022]\n",
    "df_ws2022.drop('year', axis = 'columns', inplace = True)\n",
    "df_ws2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71cac4b",
   "metadata": {},
   "source": [
    "### Handling missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01ca500",
   "metadata": {},
   "source": [
    "Now let's handle the missing values in the dataset. It is important for later analysis, connecting the weather data with BTS stations and modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4117fe1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum (df_ws2022.groupby(['id', 'name', 'lat', 'long']).count().month < 6)\n",
    "# We see that for 20 stations we do not every at least one parameter value for all months.\n",
    "\n",
    "\n",
    "# For now, let's create new rows for those months and fill them with NAs\n",
    "\n",
    "all_combinations = pd.MultiIndex.from_product([df_ws2022['id'].unique(), range(1, 13)], names = ['id', 'month'])\n",
    "all_data = pd.DataFrame(index = all_combinations).reset_index()\n",
    "\n",
    "df_ws2022 = pd.merge(all_data, df_ws2022, on = ['id', 'month'], how = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ce9fb7",
   "metadata": {},
   "source": [
    "Let's check the correlations between variables - we are working here on mean values for months, as standard deviation have different interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4abab13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df_ws2022[['A_mean', 'B_mean', 'C_mean', 'D_mean', 'E_mean']].corr()\n",
    "corr.style.background_gradient(cmap = 'coolwarm', axis=None)\n",
    "\n",
    "# As one could predict, A (air temperature) and B (ground temperature) are very highly correlated. \n",
    "# Therefore, parameter B is not that important for our analysis, as long as we have value of parameter B and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e2fc2cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's explore further how missing value structure looks like\n",
    "\n",
    "import missingno as mno\n",
    "\n",
    "(df_ws2022.isna().sum(axis=1) > 0).sum() / len(df_ws2022)\n",
    "# We still have almost 60% of rows with at least one missing value of parameter\n",
    "\n",
    "mno.matrix(df_ws2022[['A_mean', 'B_mean', 'C_mean', 'D_mean', 'E_mean']])\n",
    "# We also see that most observations with missing values have every observation missing except for D parameter value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5f0a6d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check number of missing values by ID\n",
    "\n",
    "NAs_by_ID = df_ws2022.drop(['id', 'name', 'lat', 'long'], 1).isna().groupby(df_ws2022.id, sort = False).sum(numeric_only = True).reset_index()\n",
    "NAs_by_ID['unhelpful_parameters'] = (NAs_by_ID.iloc[:, 2:] == 12).sum(axis=1) / 2 # Count how many parameters are missing for all months for specific ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f033659d",
   "metadata": {},
   "source": [
    "Weather stations for which we have data for only one or two parameters for the whole year (i.e. for a particular parameter they have NAs in all months) are little helpful in our analysis. In theory, we could extrapolate the value of missing parameters, for example by looking at other nearby stations or trying to derive it from the values of other parameters. However, it would be complicated and time-consuming and/or without the guarantee of good results (correlations are not that high except for A and B). I also checked if maybe we can subsitute it with data from the 2017, but from my brief checking (code not included here, I just changed the selected year above and performed the same calculations) we wouldn't get much more information. \n",
    "\n",
    "Therefore I decided to drop every station that have less than 4 out of 5 parameters (or 3 of 4 if we assume that A and B are carring the same information)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a33919d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check for how many observations we have less than 4 out of 5 parameters present - we are going to delete those \n",
    "\n",
    "sum(NAs_by_ID['unhelpful_parameters'] >= 3) # Unfortunately it is 300 stations which is more than a half of all our data :( \n",
    "\n",
    "# As we have seen in correlogram A can subsitute for B and vice versa. Let's check if we could have such situation. \n",
    "NAs_by_ID[(NAs_by_ID['A_mean'] + NAs_by_ID['B_mean']) == 12]\n",
    "good_A_B = np.array(NAs_by_ID[(NAs_by_ID['A_mean'] + NAs_by_ID['B_mean']) == 12]['id'].drop(NAs_by_ID.index[137]))\n",
    "\n",
    "# Deleting stations with more than 3 unhelpful parameters, except these exceptions above\n",
    "bad_parameters = np.array(NAs_by_ID[NAs_by_ID['unhelpful_parameters'] >= 3]['id'])\n",
    "df_ws2022d = df_ws2022[~df_ws2022['id'].isin(bad_parameters) | df_ws2022['id'].isin(good_A_B)].copy()\n",
    "df_ws2022d.reset_index(inplace = True)\n",
    "\n",
    "(df_ws2022d.isna().sum(axis=1) > 0).sum() / len(df_ws2022d)\n",
    "# Now we only have around 10% of parameters missing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bbdc81",
   "metadata": {},
   "source": [
    "The missing values for A can be extrapolated from missing values of B and vice versa. Here I am using, well, maybe not the most correct approach but simple and I would say effective enough for those few observations (I could do PCA and take the first principal component as some general indicator of temperature, but it would be more complex and I would prefer for now to stick with original values). \n",
    "\n",
    "I check how, in most cases, the relationship between A and B looks like and infer the value of one of the missing parameters from it if possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ac270e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_mean = (df_ws2022d['A_mean'] / df_ws2022d['B_mean']).median()\n",
    "coef_std = (df_ws2022d['A_std'] / df_ws2022d['B_std']).median()\n",
    "\n",
    "df_ws2022d['A_mean'] = df_ws2022d.apply(lambda row: row['B_mean']*coef_mean if np.isnan(row['A_mean']) else row['A_mean'], axis=1)\n",
    "df_ws2022d['B_mean'] = df_ws2022d.apply(lambda row: row['A_mean']/coef_mean if np.isnan(row['B_mean']) else row['B_mean'], axis=1)\n",
    "\n",
    "df_ws2022d['A_std'] = df_ws2022d.apply(lambda row: row['B_std']*coef_mean if np.isnan(row['A_std']) else row['A_std'], axis=1)\n",
    "df_ws2022d['B_std'] = df_ws2022d.apply(lambda row: row['A_std']/coef_mean if np.isnan(row['B_std']) else row['B_std'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef20398c",
   "metadata": {},
   "source": [
    "For the rest of the missing values, I fill them based with median value for a particular parameter for Poland for a particular month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7ccd6004",
   "metadata": {},
   "outputs": [],
   "source": [
    "month_medians = df_ws2022d.groupby('month').median(numeric_only = True)\n",
    "\n",
    "def fill_NAs(row, variable):\n",
    "    if pd.isnull(row[variable]):\n",
    "        return month_medians.loc[row['month'], variable]\n",
    "    else:\n",
    "        return row[variable]\n",
    "\n",
    "for variable in df_ws2022d.columns[6:]:\n",
    "    df_ws2022d[variable] = df_ws2022d.apply(lambda row: fill_NAs(row, variable), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8788503d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ws2022d[df_ws2022d.isna().any(axis=1)] \n",
    "# There were some missing Name, Lat and Lot values from the previous operation of filling in NAs for missing months.\n",
    "# I checked them and they are okay, not the result of an error, so I am filling them in.\n",
    "\n",
    "df_ws2022d.fillna(method='ffill', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "64291ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ws2022d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8254799",
   "metadata": {},
   "source": [
    "### Loading the data on BTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc7448c",
   "metadata": {},
   "source": [
    "Loading the data from bts.cv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0e8b05c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_bts = os. getcwd() + '/data/bts.csv'\n",
    "\n",
    "df_bts = pd.read_csv(path_bts, decimal = ',', encoding = \"utf-8\")\n",
    "df_bts.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b0cd9afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing column names and types\n",
    "\n",
    "df_bts.columns = [\"bts_id\", \"provider\", \"technology\", \"location\"]\n",
    "df_bts = df_bts.astype({'provider': 'str', 'technology': 'str', 'location': 'str'})\n",
    "df_bts.standard = df_bts.technology.str.strip()\n",
    "df_bts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4817a93",
   "metadata": {},
   "source": [
    "Creating binary variables representing technologies supported by stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "59225af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bts['technology'] = df_bts['technology'].apply(lambda x: x.split())\n",
    "df_bts\n",
    "unique_names = set(name for names in df_bts['technology'] for name in names)\n",
    "\n",
    "for name in unique_names:\n",
    "    df_bts[name] = df_bts['technology'].apply(lambda x: 1 if name in x else 0)\n",
    "\n",
    "df = df_bts.drop(['technology'], axis=1)\n",
    "df_bts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45685ba9",
   "metadata": {},
   "source": [
    "Loading the coordinates of cities where BTS stations are located\n",
    "\n",
    "Unfortunately, due to large amount of locations to process, mapping all coordinates using Geopy took quite a long time and quickly I got timeout. This is why my primarly method of mapping coordinates of BTS stations was using an exteranl source - the website https://astronomia.zagan.pl/art/wspolrzedne.html. I copy pasted the data to Excel and tranformed it there as it was much quicker and simpler than direct webscrapping. Next, I loaded this Excel file miejscowosci_lista.csv  here and did the coordinates mapping. Only when there was some problem with mergining the data I used Geopy.\n",
    "\n",
    "Some of the city names in the bts.csv file were ambiguous as in Poland we can have more than one city with the same name (there are unique only to powiats). In this case I assumed that I can assign the first coordinates by alphabetical order of powiat name that the city belongs to. \n",
    "\n",
    "I saved the data in miejscowosci_lista.csv. file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3099b653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the preprepared file based on information from the website https://astronomia.zagan.pl/art/wspolrzedne.html\n",
    "\n",
    "path_cities = os. getcwd() + '\\\\miejscowosci_lista.csv'\n",
    "\n",
    "df_cities_list = pd.read_csv(path_cities, encoding = \"utf-8\")\n",
    "df_cities_list\n",
    "df_cities_list.drop_duplicates(subset = ['location'], inplace = True) # Powiat names were deleted from location names before, here I delete duplicates created "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9fe1672f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing data type\n",
    "\n",
    "df_cities_list = df_cities_list.astype({'location': 'str', 'long': 'str', 'lat': 'str'})\n",
    "df_cities_list\n",
    "\n",
    "# Ujednolicanie nazw i zapisu\n",
    "\n",
    "df_cities_list.location = df_cities_list.location.str.strip()\n",
    "df_bts.location = df_bts.location.str.strip()\n",
    "\n",
    "df_cities_list = df_cities_list.replace('°', '.', regex = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa7e854",
   "metadata": {},
   "source": [
    "Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6c7bed08",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bts_coor = pd.merge(df_bts, df_cities_list, on ='location', how = 'left')\n",
    "df_bts_coor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0ddd5c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If location could not be found, I used Geopy mapping\n",
    "\n",
    "lista_miast = df_bts_coor[df_bts_coor.long.isnull()].location.unique()\n",
    "\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "geolocator = Nominatim(user_agent = \"request_Monika\")\n",
    "\n",
    "latitudes = [geolocator.geocode(miasto + ', Polska').latitude for miasto in lista_miast]\n",
    "longitudes = [geolocator.geocode(miasto + ', Polska').longitude for miasto in lista_miast]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "04646eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "geopy_mapped = pd.DataFrame({'location': lista_miast, 'long2': longitudes, 'lat2': latitudes})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551e4ba0",
   "metadata": {},
   "source": [
    "Merging coordinates data from the website and from the geopy mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "41a69ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bts_coor_merged = pd.merge(df_bts_coor, geopy_mapped, on ='location', how ='left') \n",
    "\n",
    "df_bts_coor_merged['long'].fillna(df_bts_coor_merged['long2'], inplace = True)\n",
    "df_bts_coor_merged['lat'].fillna(df_bts_coor_merged['lat2'], inplace = True)\n",
    "df_bts_coor_merged = df_bts_coor_merged.drop(['lat2', 'long2'], axis = 1)\n",
    "df_bts_coor_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bdf406",
   "metadata": {},
   "source": [
    "### Assigning BTS stations data on the weather"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81f6b24",
   "metadata": {},
   "source": [
    "We have already loaded the data on weather stations and data on BTS stations. To analyze the relationship between the weather and BTS station malfuntioning, we need to assing every BTS station a weather stations from which we can infer the weather at the location of this particular BTS station. \n",
    "\n",
    "Of course, we could assign more than one weather station to the BTS station and take, let's say a mean of weather variables weighted by the distance of particular weather station to that weather location. However, it seems like as unnecceary complication given the time and scope of this project. Although assigning each BTS station just one weather station means that we will have multiple BTS stations with the same weather data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0462a8dd",
   "metadata": {},
   "source": [
    "Let's prepare df with weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "76e55ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ujednolicenie współrzędnych do systemu dziesiętnego\n",
    "def coordinate_split(x):\n",
    "    deg, minutes, seconds =  x.strip().split()\n",
    "    return (float(deg) + float(minutes)/60 + float(seconds)/(60*60))\n",
    "\n",
    "df_ws2022d[['lat', 'long']] = df_ws2022d[['lat', 'long']].applymap(coordinate_split)\n",
    "\n",
    "stacje_pogoda = df_ws2022d[['id', 'lat', 'long']].drop_duplicates(ignore_index = True)\n",
    "stacje_pogoda = stacje_pogoda.astype({'lat': 'float', 'long': 'float'})\n",
    "\n",
    "df_bts_coor_merged = df_bts_coor_merged.astype({'long': 'float', 'lat': 'float'})\n",
    "df_bts_coor_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c44903",
   "metadata": {},
   "source": [
    "Let's calculate the distance between weather stations and BTS stations using the Haversin formular. Next, each BTS station we will asssign weather station that is in the closest distance to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4cb14a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import radians, sin, cos, sqrt, atan2\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    R = 6371  # Radius of the Earth in kilometers\n",
    "    dlat = radians(lat2 - lat1)\n",
    "    dlon = radians(lon2 - lon1)\n",
    "    a = sin(dlat / 2) ** 2 + cos(radians(lat1)) * cos(radians(lat2)) * sin(dlon / 2) ** 2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "    distance = R * c\n",
    "    return distance\n",
    "\n",
    "# Function to find the nearest location in df2 for a given location in df1\n",
    "def find_nearest_location(row):\n",
    "    distances = stacje_pogoda.apply(lambda x: haversine(row['lat'], row['long'], x['lat'], x['long']), axis=1)\n",
    "    min_distance_index = distances.idxmin()\n",
    "    nearest_location = int(stacje_pogoda.loc[min_distance_index, 'id'])\n",
    "    min_distance = distances[min_distance_index]\n",
    "    return nearest_location, min_distance  \n",
    "\n",
    "df_bts_coor_merged[['near_w_station', 'dist_near_w_station']] = df_bts_coor_merged.apply(find_nearest_location, axis=1, result_type='expand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5c3d94d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_bts_coor_merged['dist_near_w_station'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c6e3ae93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_bts_coor_merged['near_w_station'].value_counts().hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92ed204",
   "metadata": {},
   "source": [
    "We see that the assigment went quite okay - most BTS stations are within 50 km of their assigned weather stations, there are few examples above that. Moreover, as we have more BTS stations than weather stations, some weather stations were assigned to more than 250 weather stations. However, it would be hard to avoid such situation, althought it would be interesting to check later, how it corresponds with anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "51146a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the data - each BTS station we assigned the nearest location of weather station. \n",
    "# Now, we transfer the corresponding data on the weather to each BTS station.\n",
    "\n",
    "df_bts_weather = pd.merge(df_bts_coor_merged, df_ws2022d, left_on = 'near_w_station', right_on = 'id', how = 'outer')\n",
    "df_bts_weather.dropna(subset = ['bts_id'], inplace = True)\n",
    "df_bts_weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e9c35b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "df_bts_weather"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e7ca9f",
   "metadata": {},
   "source": [
    "### Adding the data on BTS malfunctioning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186547f0",
   "metadata": {},
   "source": [
    "Loading the data from bts_anomalie_pct.xlsx file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "18b3318f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_bts_anomalies = os.getcwd() + '/data/bts_anomalie_pct.xlsx'\n",
    "\n",
    "df_bts_anomalies_raw = pd.read_excel(path_bts_anomalies)\n",
    "df_bts_anomalies_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "93b9478c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's focus on the year 2022\n",
    "df_bts_anomalies_raw = df_bts_anomalies_raw.iloc[:, 13:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301ebdbc",
   "metadata": {},
   "source": [
    "Stacking for easier merging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a26326f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies_stacked = df_bts_anomalies_raw.stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bc61c73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bts_anomalies = pd.DataFrame(anomalies_stacked)\n",
    "df_bts_anomalies.reset_index(inplace = True)\n",
    "df_bts_anomalies.columns = ['bts_id', 'month', 'anomalies']\n",
    "df_bts_anomalies['month'] = df_bts_anomalies['month'].str.split('-').str[1].astype(int)\n",
    "df_bts_anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1ed4a3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkign NAs\n",
    "df_bts_anomalies.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239d9e49",
   "metadata": {},
   "source": [
    "Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "142c7ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main = pd.merge(df_bts_weather, df_bts_anomalies, how = 'left', on = ['bts_id', 'month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "aab5fe6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's delete unnessesary columns and arrange it better\n",
    "\n",
    "df_main.drop(['technology', 'day', 'index', 'near_w_station'], axis = 'columns', inplace = True)\n",
    "df_main.rename(columns = {'long_x': 'long_bts', 'lat_x': 'lat_bts', 'lat_y': 'lat_w_station', 'long_y': 'long_w_station', 'id': 'w_station_id', 'location': 'location_bts', 'name': 'location_w_station'}, inplace = True)\n",
    "df_main = df_main.astype({'bts_id': 'int', '5G': 'bool', 'UMTS': 'bool', 'IOT': 'bool', 'LTE': 'bool', 'GSM': 'bool'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c0c035eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_columns', None)\n",
    "df_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "36a52ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.reset_option('display.max_columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e9350d",
   "metadata": {},
   "source": [
    "We obtained our final dataset with all information intresting for us. Let's save it for further use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d7e9b29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main.to_pickle('df_data_final.pkl')\n",
    "df_main = pd.read_pickle('df_data_final.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c48b66",
   "metadata": {},
   "source": [
    "## Data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02aab4a5",
   "metadata": {},
   "source": [
    "At the previous stage, I already adressed the following problems:\n",
    "- data correctness - I deleted or corrected likely errors in the weather data\n",
    "- handling missing data - I deleted weather observations where number of missing values was too high; for the rest of observations in filled it in with median parameter values for a particular month. At this point we have no missing data\n",
    "- handling duplicates - they were deleted\n",
    "- choosing data scope - our data is aggregated at the month level in term of mean values and standard deviations. I interpret mean monthly values of parameters, for example A_mean as general indicator of the variable for the paricular station and month. I interpret montly standard deviations of parameters, for example A_std as indicator of variability of the variable for the paricular station and month. \n",
    "\n",
    "Now we can proceed to the data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4dbc7fc",
   "metadata": {},
   "source": [
    "### Preliminary analysis for the weather stations data\n",
    "\n",
    "Taking into account our main goal i.e. analyzing the relationship between the weather and BTS station malfuntioning, we are interested only in the weather data from the weather stations that were \"assigned\" to one of the BTS stations. We could theoretically analyze all the stations across Poland and the results would be probably similiar, but for clarity let's focus only on weather stations assigned to BTS stations.\n",
    "\n",
    "At some points during preprocessing we already calculated basic statistics and correlations, but let's do it again for the weather data in the final dataset. \n",
    "\n",
    "(*I am aware that after aggregation, we obtained montly means and stds based on separate hour values i.e. our raw data. Now, we will be calulaing for example, mean of those means which probably won't be exactly equal to the mean which could be calculated based on observations for separate hours, the same for standard deviations etc.). However, I treat it as good enough approximation for later analysis and those variables still have explanotary power*)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb04021",
   "metadata": {},
   "source": [
    "Exctracting the weather data from the main file, so that we do not count duplicate stations as separate observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d969e114",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather = df_main.drop_duplicates(subset = ['w_station_id', 'month']).reset_index(drop = True).iloc[:, 11:]\n",
    "df_weather"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edaea4a",
   "metadata": {},
   "source": [
    "**Basic statistics** of weather variables and BTS malfunctioning (anomalies) across all months and all BTS stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "786e5c5c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_weather' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdf_weather\u001b[49m\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m4\u001b[39m:]\u001b[38;5;241m.\u001b[39mdescribe()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_weather' is not defined"
     ]
    }
   ],
   "source": [
    "df_weather.iloc[:, 4:].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca113cd5",
   "metadata": {},
   "source": [
    "Looking at the basic statistics, we can also see high correlation or likeliness between A and B parameters. Values of all parameters seem quite likely in terms of correspondence with real life.\n",
    "\n",
    "Now let's look at the **Pearson correlations** for the weather variables (numerical)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8bd440ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df_weather.iloc[:, 4:].corr()\n",
    "corr.style.background_gradient(cmap = 'coolwarm', axis=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6caeb8da",
   "metadata": {},
   "source": [
    "As we are anlyzing the weather data, we see some obvious relationships between variables. Most of them could be probably explained by science, if one is a weather expert. For example, wind conditions and humidity are generally dependend on the temperature and pressure of the system. At the first glance, we see very high positive linear correlations between A (air temperature) and B (ground temperature) variables, both between their mean values and between their standard deviations (which is of course expected). For stds, we have high negative correlation for B_std and E_std, which is interesting. The rest of stds are not that significantly correlated. For the rest, correlations are not that strong.\n",
    "\n",
    "Moreover, there are quite significant correlations between a particular variable mean and its standard deviation. Although under normal distribution, mean and standard deviation should be independent, here we observe that for variables A, B, C and D the higher the value of the parameter in a particular month, the higher its variability. For variable E it is the opposite.\n",
    "\n",
    "Unfortunately, all analyzed variables are not at all linearly correlated with percent of anomalies, which is our target variable. It suggest that doing simple linear regression on weather variables to predict the anomalies might not be sufficient. However, we still not taken into account categorical variables, such as months or stations itself. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efa5f3d",
   "metadata": {},
   "source": [
    "Now let's look at **mean air temperature and its variability** across the months at the level of Poland (all BTS stations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "27597fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker    \n",
    "import calendar\n",
    "\n",
    "month_labels = [calendar.month_abbr[x] for x in range(1, 13)]\n",
    "\n",
    "pom = df_weather.groupby('month').mean(numeric_only = True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "plt.plot(month_labels, pom['A_mean'], label = \"Mean monthly temperature\", color = 'blue') \n",
    "plt.plot(month_labels, pom['A_std'], label = \"Mean standard deviation of temperature throught the month\", linestyle = 'dotted')\n",
    "plt.gca().yaxis.set_major_formatter(mticker.FormatStrFormatter('%d°C'))\n",
    "plt.title(\"Temperature in BTS stations in 2022\")\n",
    "plt.legend(loc = 'lower center', bbox_to_anchor=(0.5, -0.3)) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25f0517",
   "metadata": {},
   "source": [
    "According to expectations, we see the highest montly mean temperatures of around 18-20°C in summer months: August, July and June. On the other hand, the lowest temperatures of 0°C we have in winter months: January and December. Mean standard deviation of temperatures are pretty constant throught the year and are of around 3-5°C."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4437ff1f",
   "metadata": {},
   "source": [
    "Now let's look at mean **wind speed and total daily percipitation** across the months at the Poland level (all BTS stations). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a38fc916",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "plt.plot(month_labels, pom['C_mean'], label = \"Mean monthly wind speed\", color = 'black') \n",
    "plt.bar(month_labels, pom['D_mean'], label = \"Mean daily rainfall during the month\")\n",
    "plt.ylabel(\"Wind in km/h or rainfall in mm\") # Założyłam jednostki\n",
    "plt.title(\"Weather variables measured by weather stations in 2022\")\n",
    "plt.legend(loc = 'lower center', bbox_to_anchor=(0.5, -0.3)) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ac785d",
   "metadata": {},
   "source": [
    "We see the highest daily percipitation in July and September and the lowest in March. The wind is the strongest in winter, in January and February."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36dbad6",
   "metadata": {},
   "source": [
    "### Preliminary analysis for the BTS stations data\n",
    "\n",
    "Let's focus now on the BTS stations data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dad26e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main.provider.value_counts().plot(kind = 'barh', title = 'Number of BTS stations by provider')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7fb06bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as mtick\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "\n",
    "technologies = ['LTE', 'GSM', 'UMTS', '5G', 'IOT']\n",
    "percentages = [(df_main[technology].value_counts(normalize = True) * 100)[1] for technology in technologies]\n",
    "\n",
    "plt.barh(technologies, percentages)\n",
    "plt.gca().xaxis.set_major_formatter(mtick.PercentFormatter())\n",
    "plt.title(\"Percentage of all BTS stations using particular technology\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e2d51b",
   "metadata": {},
   "source": [
    "Also, let's look at the distribution of BTS failure rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0a2b7896",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main.anomalies.hist(bins = 16)\n",
    "plt.xlabel('Percentage of anomalies')\n",
    "plt.ylabel('Nuber of observations')\n",
    "plt.title('Distribution of percentage of anomalies across BTS stations')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba09bf7",
   "metadata": {},
   "source": [
    "Well, as it was said, the data was generated randomly which could probably explain the uniform distribution of anomalies. All the possible values range from 0.00 to 0.15 by 0.01. In real life, one would probably expect something more normal looking, for example that a high rates of anomalies would be less frequent. \n",
    "\n",
    "We can also check if generally anomalies could be dependent on the month using one-way ANOVA, but we see that they are not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "72b0eb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies_by_month = df_main[['month', 'anomalies']]\n",
    "\n",
    "anova_abm = stats.f_oneway(*[group['anomalies'] for name, group in anomalies_by_month.groupby('month')])\n",
    "print(anova_abm.pvalue)\n",
    "\n",
    "# P-value is much higher than 0.05 so we cannot reject the null hypothesis that the mean values of anomalies are statistically equal across the months."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c830fe44",
   "metadata": {},
   "source": [
    "### Comparing Orange and other providers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb1716b",
   "metadata": {},
   "source": [
    "Let's compare anomalies for Orange stations and its competitors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "520c7fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "providers = df_main.drop(['bts_id', 'long_bts', 'lat_bts', 'month', 'w_station_id'], axis = \"columns\").groupby('provider').mean()\n",
    "providers['bts_id_count'] = df_main.provider.value_counts()\n",
    "providers\n",
    "\n",
    "# The table is pretty straightfoward, so I feel no need to visualize it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634295e0",
   "metadata": {},
   "source": [
    "Looking at the table, we can see that all providers have, in fact, very similiars networks in distributions of technologies they use and weather conditions at the stations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf974632",
   "metadata": {},
   "source": [
    "### Identification of stations that are exposed to the most extreme weather changes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedec2fd",
   "metadata": {},
   "source": [
    "As I aggregated data on the monthly levels, we will focus on those statistics; however, I acklowledge that it could be also beneficial to have the data at lower level of aggregation, for example to check the number of separate extreme weather events throught the month that usually last few hours. Still, as standard deviation was calculated based on those raw single observation on the level of hours, we have some information preserved. I am focusing here on standard deviation as an indicator of weather changes/its variability.\n",
    "\n",
    "Here I am first identifing **weather stations** among those assigned to BTS, so using the df_weather dataframe. As was showned before, each weather station can have up to 250 BTS stations assigned to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ee65f298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The stations with largest A_std are actually the same as the largest variations in B_std\n",
    "\n",
    "top3_AB_std = df_weather.nlargest(3, 'A_std')\n",
    "top3_AB_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8cbe7119",
   "metadata": {},
   "outputs": [],
   "source": [
    "top3_C_std = df_weather.nlargest(3, 'C_std')\n",
    "top3_C_std\n",
    "# The largest variation is for Śnieżka which could be explained by its special location in the mountains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1a1cedf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "top3_D_std = df_weather.nlargest(3, 'D_std')\n",
    "top3_D_std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e97dd13",
   "metadata": {},
   "source": [
    "Now let's get all BTS stations that are assigned to one of the above weather stations identified as those with the highest variability of weather variables. We see that we have quite a lot of those stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e12d8f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_changes_stations = list(set(top3_AB_std['w_station_id'].to_list() + top3_C_std['w_station_id'].to_list() + top3_D_std['w_station_id'].to_list() ))\n",
    "\n",
    "filtered_data = df_main[df_main['w_station_id'].isin(weather_changes_stations)]\n",
    "bts_ids_weather_changes = list(set(filtered_data['bts_id']))\n",
    "print(*bts_ids_weather_changes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29534f2d",
   "metadata": {},
   "source": [
    "### Visualization of stations on the map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e0d74b",
   "metadata": {},
   "source": [
    "Loading the map of Poland with voivodeships borders. I downloaded the shapefile from https://gis-support.pl/baza-wiedzy-2/dane-do-pobrania/granice-administracyjne/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "75369bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd \n",
    "\n",
    "path = os.getcwd() + '\\wojewodztwa\\wojewodztwa.shp'\n",
    "\n",
    "polska = gpd.read_file(path).to_crs(epsg = 4326)\n",
    "polska.crs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d0302f",
   "metadata": {},
   "source": [
    "Connecting the points from our dataframe to points on the map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a0d3d0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main_bts = df_main.copy()\n",
    "df_main_ws = df_main.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2f6bce83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Point\n",
    "\n",
    "geometry_bts = [Point(xy) for xy in zip(df_main_bts['long_bts'], df_main_bts['lat_bts'])]\n",
    "geo_df_bts = gpd.GeoDataFrame(df_main_bts, geometry = geometry_bts, crs = 'epsg:4326')\n",
    "\n",
    "geometry_w_station = [Point(xy) for xy in zip(df_main_ws['long_w_station'], df_main_ws['lat_w_station'])]\n",
    "geo_df_w_station = gpd.GeoDataFrame(df_main_ws, geometry = geometry_w_station, crs= 'epsg:4326')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009cf847",
   "metadata": {},
   "source": [
    "Now, let's visualize everything on the maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ba1974b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (7, 7))\n",
    "\n",
    "polska.plot(ax = ax, color = 'lightgrey', edgecolor='white', linewidth=1)\n",
    "\n",
    "geo_df_bts.plot(ax = ax, color = 'deepskyblue', marker = 'o', markersize = 7, label = 'BTS')\n",
    "geo_df_w_station.plot(ax = ax, color = 'tomato', marker = '^', markersize = 9, label = 'Weather Station')\n",
    "\n",
    "ax.set_title('BTS and Weather Station Locations')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e4b52d",
   "metadata": {},
   "source": [
    "We see that the BTS stations are actually grouped in quite visible groups. Also we see some locations even outside Poland. Let's remind that this are locations whose coordinates we had to map ourself using city names. This might be due to artificial generation of data, actual BTS stations characteristics or - I hope not because I have checked it - possible problems with mapping :)\n",
    "\n",
    "On the other hand, weather stations seems to be placed at random. However, let's remind that during the initial phrase we dropped almost half of weather stations due to insufficient data, and then some of the remaining weather station might have not been used as they were not close to any BTS station. So we do not know the initial distribution of those stations across the map."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06eb681",
   "metadata": {},
   "source": [
    "Let's also try to visualize some weather parameters. For example, let's visualize which weather station experience the highest temperatures during August (the warmest month)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0f0b12b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as mcolors\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "polska.plot(ax = ax, color='lightgrey', edgecolor = 'white', linewidth = 1)\n",
    "\n",
    "df_month_8 = df_main[df_main['month'] == 8]\n",
    "\n",
    "max_a_mean_month_8 = df_month_8.groupby('w_station_id')['A_mean'].max()\n",
    "geo_df_w_station = geo_df_w_station.merge(max_a_mean_month_8, left_on='w_station_id', right_index=True, suffixes=('', '_max'))\n",
    "\n",
    "# Plot Weather Stations with colors based on A_mean_max\n",
    "cmap = plt.cm.get_cmap('Reds')\n",
    "normalize = mcolors.Normalize(vmin=geo_df_w_station['A_mean_max'].min(), vmax=geo_df_w_station['A_mean_max'].max())\n",
    "colors = [cmap(normalize(value)) for value in geo_df_w_station['A_mean_max']]\n",
    "\n",
    "geo_df_w_station.plot(ax = ax, color = colors, marker = 'o', markersize = 14)\n",
    "\n",
    "# Creating custom color map legend\n",
    "sm = plt.cm.ScalarMappable(cmap = cmap, norm = normalize)\n",
    "sm.set_array([]) \n",
    "cbar = plt.colorbar(sm, ax=ax, orientation = 'vertical', fraction = 0.03, pad = 0.1)\n",
    "cbar.set_label('Mean temperature')\n",
    "\n",
    "ax.set_title('Mean temperature for weather stations in August')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9737db1e",
   "metadata": {},
   "source": [
    "### Assesing the impact of weather on BTS malfunctioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4021aded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into dependent and independ variables\n",
    "\n",
    "X = df_main[['bts_id', 'month', 'UMTS', 'IOT', 'LTE', '5G', 'GSM', 'A_mean', 'C_mean', 'D_mean', 'E_mean', 'A_std', 'C_std', 'D_std', 'E_std']].set_index(['bts_id', 'month']) #Not including B\n",
    "X = pd.get_dummies(data = X, columns = ['UMTS', 'IOT', 'LTE', '5G', 'GSM'], drop_first = True)\n",
    "Y = df_main['anomalies']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c33a7c",
   "metadata": {},
   "source": [
    "In this project we are dealing with the panel data which means that we have multiple units which we observe at multiple points in time. From the statistical perpective, panel data could be harder to deal with due to autocorrelation of errors from the time dimension. Therefore, it is better to use regressions models specifically dedicated to panel data.\n",
    "\n",
    "There are three main panel data models: pooled OLS, fixed effects and random effects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcf999b",
   "metadata": {},
   "source": [
    "Let's start with **pooled OLS** which is the most basic one and corresponds to standard OLS, just done on the panel data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "71f78a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_panel = df_main[['bts_id', 'month', 'UMTS', 'IOT', 'LTE', '5G', 'GSM', 'A_mean', 'C_mean', 'D_mean', 'E_mean', 'A_std', 'C_std', 'D_std', 'E_std', 'anomalies']].set_index(['bts_id', 'month'])\n",
    "df_panel = pd.get_dummies(data = df_panel, columns = ['UMTS', 'IOT', 'LTE', '5G', 'GSM'], drop_first = True)\n",
    "\n",
    "months = df_panel.index.get_level_values('month').to_list()\n",
    "df_panel['months'] = pd.Categorical(months)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "528dd185",
   "metadata": {},
   "outputs": [],
   "source": [
    "from linearmodels import PooledOLS\n",
    "import statsmodels.api as sm\n",
    "\n",
    "X = df_panel.loc[:, df_panel.columns != \"anomalies\"]\n",
    "X = sm.add_constant(X)\n",
    "Y = df_panel['anomalies']\n",
    "\n",
    "mod = PooledOLS(Y, X)\n",
    "pooledOLS_res = mod.fit(cov_type = 'clustered', cluster_entity=True)\n",
    "\n",
    "# Store values for checking homoskedasticity graphically\n",
    "fittedvals_pooled_OLS = pooledOLS_res.predict().fitted_values\n",
    "residuals_pooled_OLS = pooledOLS_res.resids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e9054e",
   "metadata": {},
   "source": [
    "Now let's check whether this model is correct. For Pooled OLS to be correct we have to have:\n",
    "- homoskedasticity of errors (constant variance) \n",
    "- no autocorrelations of erros (no correlations between errors in different terms)\n",
    "\n",
    "Let's check homoskedasticity first using visualization and statistical tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3e3daba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting errors for visual inspection\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(fittedvals_pooled_OLS, residuals_pooled_OLS, color = 'blue')\n",
    "ax.axhline(0, color = 'r', ls = '--')\n",
    "ax.set_xlabel('Predicted Values', fontsize = 15)\n",
    "ax.set_ylabel('Residuals', fontsize = 15)\n",
    "ax.set_title('Homoskedasticity Test', fontsize = 30)\n",
    "plt.show()\n",
    "\n",
    "# Errors seems to be equally distributed (as equal as generated data can be)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6f7028b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's check homoskedasticity using the formal tests\n",
    "\n",
    "from statsmodels.stats.diagnostic import het_white, het_breuschpagan\n",
    "\n",
    "# White-Test\n",
    "white_test_results = het_white(pooledOLS_res.resids, X)\n",
    "labels = ['LM-Stat', 'LM p-val', 'F-Stat', 'F p-val'] \n",
    "print(dict(zip(labels, white_test_results)))\n",
    "\n",
    "# Breusch-Pagan-Test\n",
    "breusch_pagan_test_results = het_breuschpagan(pooledOLS_res.resids, X)\n",
    "print(dict(zip(labels, breusch_pagan_test_results)))\n",
    "\n",
    "# In both tests p-value is higher than 0.05 -> we cannot reject the null hypothesis that errors are homoskedastic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196c661b",
   "metadata": {},
   "source": [
    "Let's also check the autocorrelation using Durbin-Watson test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b300028b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.stattools import durbin_watson\n",
    "\n",
    "# Durbin-Watson test\n",
    "durbin_watson_test_results = durbin_watson(pooledOLS_res.resids) \n",
    "print(durbin_watson_test_results)\n",
    "\n",
    "# The value of Durbin-Watson test is very close to 2 which suggests no autocorrelation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6fdeed",
   "metadata": {},
   "source": [
    "Suprisingly, we found that the Pooled OLS model fulfill approriate assumptions. Suprisingly because it is very hard to fullfill these assumptions on real-world data, but here it can be explained since the data was generated. Therefore, our Pooled OLS is correct and could derive statistical inference. It could be beneficial to compare Pooled OLS also with other models (fixed and random effects) but due to lack of time, let's stick with that.\n",
    "\n",
    "Let's check the summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8df0f45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pooledOLS_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7384dd53",
   "metadata": {},
   "source": [
    "It seems that all the variables are actually insignificant! They all have p-values greater than 0.05, and much greater in most cases. This also includes all weather variables. We have seen from the correlation table that, in fact, there are little linear relationship between them and the anomalies. However, I still hoped than when controlling for other variables, some relationship could become prominent. \n",
    "\n",
    "In this case we might try to also add square roots of weather variables - maybe anomalies are more prominent in extreme weather?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "aebcbefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding square roots of variables\n",
    "\n",
    "# I tried to did that code but got error \"exog does not have full column rank\" which suggest that the created columns were linear combiantions of other coluns\n",
    "\n",
    "# columns_to_square = ['A_mean', 'C_mean', 'D_mean', 'E_mean']\n",
    "# X2 = sm.add_constant(X.assign(**{f'{col}^2': lambda x: x[col] ** 2 for col in columns_to_square}))\n",
    "\n",
    "X['A_mean2'] = X['A_mean']**2\n",
    "# I also tried other variables separately, but only A was stastically significant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "121acfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a new model\n",
    "\n",
    "mod2 = PooledOLS(Y, X)\n",
    "pooledOLS_res2 = mod2.fit(cov_type = 'clustered', cluster_entity=True)\n",
    "\n",
    "# I do present here checking the assumptions once again as the data has not changed that much. All assumptions still hold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9bc56e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pooledOLS_res2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8933370",
   "metadata": {},
   "source": [
    "We see that only the air temperature (variable A) and its square root is significant with p-values below 0.05 (as mentioned in the comments, I also tested adding square roots of other variables but they were insignificant). However, its coefficient is very small which suggests that still, temperature does not affect anomalies in any significant way. Also, the April (months.4) variable became significant, but at the margin (0.0489) and in previous regression it was pretty insignificant, so I would say this is within statistical error. Moreover, the R^2 of the model is very small as well. \n",
    "\n",
    "To sum up, based on the pooled OLS models I assess that **the weather (and other variables, such as months or technology) have no relationship with anomalies in BTS stations** (I am using pooled OLS so from the stastical perpective I cannot say anything about the casual relationship between those variables)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d07305",
   "metadata": {},
   "source": [
    "## Training machine learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c82da3e",
   "metadata": {},
   "source": [
    "### Clustering - identification of weather stations with similiar conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec73f78",
   "metadata": {},
   "source": [
    "As I mentioned before, each weather station has assigned multiple BTS stations. Therefore, clustering BTS stations based on weather conditions would be quite poitless, as obviously all BTS stations which were assigned the same weather stations would be clustered together. \n",
    "\n",
    "This is why I decided to cluster only the weather stations. If it would be neccesary, one can easily extract BTS stations assigned to weather stations in each cluster. Although we are losing the data on anomalies, well - as we have seen before there is little relationship between the weather and anomalies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107d7683",
   "metadata": {},
   "source": [
    "#### Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "effcda86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are using the dataframe df_weather created earlier. Let's unstack it and create new variables for each month.\n",
    "\n",
    "df_weather_clus_pom = df_weather.drop(['location_w_station', 'lat_w_station', 'long_w_station', 'anomalies'], axis = 1)\n",
    "\n",
    "df_weather_clus = df_weather_clus_pom.pivot(index = ['w_station_id'], columns = ['month'])\n",
    "df_weather_clus.columns = ['_'.join(map(str, col)).strip() for col in df_weather_clus.columns.values]\n",
    "df_weather_clus.reset_index(inplace=True)\n",
    "\n",
    "df_weather_clus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f3191b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's select the features\n",
    "\n",
    "X = df_weather_clus.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ee102e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's normalize them\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "std_scale = preprocessing.StandardScaler().fit(X)\n",
    "X = std_scale.transform(X)\n",
    "normalized_X = pd.DataFrame(X, columns = df_weather_clus.iloc[:, 1:].columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc826e0",
   "metadata": {},
   "source": [
    "By unstacking monthly data we have obtained looots of features. I tried to reduce their number, but correlations between them were not obvious. Additionaly, when I tried the PCA, I got that \n",
    "\n",
    "We need to reduce the number of features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "38a7ce6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix is too large to nicely display so I skipped it\n",
    "\n",
    "# corr_clus = normalized_X.corr()\n",
    "# corr_clus.style.background_gradient(cmap = 'coolwarm', axis=None)\n",
    "\n",
    "# Briefly looking at this large matrix we see that unfortunately not all variables are correlated with each other across one\n",
    "# variable type (so for example E_std_10 and E_std_8 have quite low correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b82d9d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Here I tried to perform PCA. However, unfortunately I did not have time to properly interpret the principal components and use them in the further analysis, so I am just presenting what I have. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d811748b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "exp_var = []\n",
    "\n",
    "for k in range(2, 20):\n",
    "\n",
    "    pca = PCA(n_components = k)\n",
    "    pca_features = pca.fit_transform(normalized_X)\n",
    "    \n",
    "    exp_var.append(pca.explained_variance_ratio_.cumsum()[-1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1d021540",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components = 20).fit(normalized_X)\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (8,5)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "xi = np.arange(1, 21, step = 1)\n",
    "y = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "plt.ylim(0.0, 1.1)\n",
    "plt.plot(xi, y, marker='o', linestyle='-', color='black')\n",
    "\n",
    "plt.xlabel('Number of Components')\n",
    "plt.xticks(np.arange(1, 21, step=1)) \n",
    "plt.ylabel('Cumulative variance (%)')\n",
    "plt.title('The number of components needed to explain variance')\n",
    "\n",
    "plt.axhline(y=0.95, color='grey', linestyle='--')\n",
    "plt.text(1.1, 1, '95% cut-off threshold', color = 'black', fontsize=16)\n",
    "\n",
    "ax.grid(axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f72272b",
   "metadata": {},
   "source": [
    "We see that to properly explain the variance in the dataset, we actually need a high number of principal components which might suggest that the PCA would not be that much helpful. However, further looking into e.g. which variables contistute each principal component is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82386278",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "As I unfortunately did not have time to implement PCA in  my solution, I did clustering on all the features. However, I am aware that in this situation my clustering might not be so good due to the noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3dfc8b7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "kmeans_kwargs = {\"init\": \"random\", \"n_init\": 10, \"max_iter\": 300, \"random_state\": 42}\n",
    "\n",
    "sse = []\n",
    "silhouette_coefficients = []\n",
    "\n",
    "for k in range(2, 15):\n",
    "    \n",
    "    kmeans = KMeans(n_clusters = k, **kmeans_kwargs)\n",
    "    kmeans.fit(normalized_X)\n",
    "    \n",
    "    sse.append(kmeans.inertia_)\n",
    "    \n",
    "    score = silhouette_score(normalized_X, kmeans.labels_)\n",
    "    silhouette_coefficients.append(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2fc8ad",
   "metadata": {},
   "source": [
    "Let's see how many clusters we should choose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6b9fdb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting bthe SSE to check the optimal number of clusters\n",
    "\n",
    "plt.plot(range(2, 15), sse)\n",
    "plt.xticks(range(2, 15))\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"Sum of Square Error (SSE)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6d06572f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chceking also the optimal number of clusters using the silhouette score\n",
    "\n",
    "plt.plot(range(2, 15), silhouette_coefficients)\n",
    "plt.xticks(range(2, 15))\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"Silhouette Coefficient\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c196c4a",
   "metadata": {},
   "source": [
    "Based on the SSE graph, there is no that much clear elbow visible. However, I would say that 7 clusters is a nice point where the SSE stops decreasing so rapidly.\n",
    "\n",
    "On the other hand, looking at the silhouette coeddicient graph, we see that our assigment into groups is not good. Low coefficient values, close to 0 suggest that the clusters are not that different and might be overlapping. We should be looking at the higherst possible silhouette scores. For previously chosen 7 clusters the value is very low but still better than for most of other cluster numbers. 4 clusters, on the other hand which have the highest silhouette score after 2, are not so great based on elbow method.\n",
    "\n",
    "This is why I would stick with 7 clusters for K-Means. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8dff96ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters = 7, **kmeans_kwargs)\n",
    "kmeans.fit(normalized_X)\n",
    "df_weather_clus['k_means_7'] = kmeans.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fa1571",
   "metadata": {},
   "source": [
    "Let's visualize clusters on the map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d3212886",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather_clus_plot = pd.merge(df_main_ws, df_weather_clus, how = 'left', on ='w_station_id' )\n",
    "df_weather_clus_plot.drop_duplicates(subset = ['w_station_id'], keep = 'last', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "146c1722",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "polska.plot(ax=ax, color='lightgrey', edgecolor='white', linewidth=1)\n",
    "\n",
    "# Plotting weather stations with cluster labels\n",
    "for cluster_label, color in zip(df_weather_clus_plot['k_means_7'].unique(), ['blue', 'green', 'red', 'purple', 'orange', 'brown', 'pink']):\n",
    "    cluster_stations = df_weather_clus_plot[df_weather_clus_plot['k_means_7'] == cluster_label]\n",
    "    ax.scatter(cluster_stations['long_w_station'], cluster_stations['lat_w_station'], label=f'Cluster {cluster_label}', color=color, marker='o', s=50)\n",
    "\n",
    "ax.set_title('Weather Stations with K-Means Clusters')\n",
    "ax.legend()\n",
    "plt.legend(loc = 'center right', bbox_to_anchor=(1.5, 0.5))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ef3a4a",
   "metadata": {},
   "source": [
    "We see that, in fact, based on geography created clusters may correspond with actual weather conditions in the stations.\n",
    "\n",
    "### Classification model predicting a high percentage of anomaly occurences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d08366",
   "metadata": {},
   "source": [
    "In this part I will try to predict if a particular BTS station will experience a high percentage anomalies. I will do it based on the weather and station data. \n",
    "\n",
    "In the Data Analysis section we already took a look at the distribution of anomalies. As a cut-of point for classifying a percentage of anomalies as high I have chosen 0.14 which is the 94th quantile. This means that I consider the 6% of the highest percentages of anomalies as a high anomaly occurence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebc3e2a",
   "metadata": {},
   "source": [
    "#### Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "df715adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy import stats\n",
    "\n",
    "# stats.percentileofscore(df_main['anomalies'], 0.14)\n",
    "# df_main['anomalies'].quantile(0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a22429c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main['high_anomaly'] = np.where(df_main['anomalies'] > 0.14, 1, 0)\n",
    "df_main['high_anomaly'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73ba51e",
   "metadata": {},
   "source": [
    "I already checked the correlation between variables in the previous steps. However, it was not very helpful. I could do a PCA for better feature selection, but due to the lack of time, I just stick with the original dataset and I only did not include B_mean and B_std due to its high correlation with A_mean and A_std."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a859e951",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_main[['A_mean', 'C_mean', 'D_mean', 'E_mean', 'A_std', 'C_std', 'D_std', 'E_std', 'UMTS', 'IOT', 'LTE', '5G', 'GSM', 'month']] # Not including B\n",
    "X = pd.get_dummies(data = X, columns = ['UMTS', 'IOT', 'LTE', '5G', 'GSM', 'month'], drop_first = True)\n",
    "Y = df_main['high_anomaly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "308d741a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into training and testing samples so that proportion of positive and negatives are rouhly the same in each sample.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3, stratify = Y, random_state = 12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "5ee756a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the data\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "num_cols = X_train.iloc[:, :8].columns\n",
    "    \n",
    "std_scale = preprocessing.StandardScaler().fit(X_train[num_cols])\n",
    "\n",
    "X_train[num_cols] = std_scale.transform(X_train[num_cols])\n",
    "X_test[num_cols]  = std_scale.transform(X_test[num_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29539433",
   "metadata": {},
   "source": [
    "We have very low percentage of positives so our models might have problems dealing with them. This is why I rebalance the training data using SMOTE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d63f232e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rebalancing training data using SMOTE\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "sm = SMOTE(random_state = 12345)\n",
    "X_train, Y_train = sm.fit_resample(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3529dc41",
   "metadata": {},
   "source": [
    "As the main evaluation metric for the models I have chosen the F2-score. F-score is a measure of model's accuracy calculated based on its precision and recall. For F2-score, recall is considered twice as important as precision. F-scores are also suitable for imbalanced data, such as in our dataset, i.e. where number of observations in one of the classes in much higher than in the other.\n",
    "\n",
    "I have chosen F2-score, because I believe that in our problem we are more concerned with recall than with precision. It would be better if our model misclassified some real negatives (real low anomalies) as positives (high anomalies) and we just did some additional checks of those stations, than if it misclassified real positives (real high anomalies) as negatives (low anomalies) and we were not prepared for that.\n",
    "\n",
    "I decided to solerly focus on the F2 score metric for easier intepretability of models' results. However, given more time and resources, one should consider also other model evaluation metrics, for exampele balanced accuracy and the ROC AUC curve. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "81bf99ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a performance metric for our model\n",
    "\n",
    "from sklearn.metrics import make_scorer, fbeta_score\n",
    "\n",
    "f2_scorer = make_scorer(fbeta_score, beta = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b252bb",
   "metadata": {},
   "source": [
    "I am using cross-validation to test the performance of the models first on the training data and to choose appropriate parameters for the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "94772d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries neccesary for cross validation and choosing optimal parameter values\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ee6f52",
   "metadata": {},
   "source": [
    "#### Logit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22baeae",
   "metadata": {},
   "source": [
    "Let's start with simple logit model as a benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6be8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "cv_5 = RepeatedKFold(n_splits = 5, random_state = 12345, n_repeats = 3)\n",
    "logistic = LogisticRegression()\n",
    "logistic_scores = cross_validate(logistic, X_train, Y_train, scoring = f2_scorer, cv = cv_5)\n",
    "\n",
    "logistic_fitted = logistic.fit(X_train, Y_train)\n",
    "print(\"F2 Score:\", logistic_scores['test_score'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34695947",
   "metadata": {},
   "source": [
    "We see that the F2 score of our logit model isn't good. \n",
    "\n",
    "We can also try some feature engineering but since we are working on the train sample, we probably won't see an improvement of performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1d9076",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "logistic_fe = make_pipeline(SelectKBest(f_classif, k = 10), LogisticRegression())\n",
    "logistic_scores_fe = cross_validate(logistic_fe, X_train, Y_train, scoring = f2_scorer, cv = cv_5)\n",
    "\n",
    "logistic_fe_fitted = logistic_fe.fit(X_train, Y_train)\n",
    "print(\"F2 Score:\", logistic_scores_fe['test_score'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bf2f6f",
   "metadata": {},
   "source": [
    "Indeed, our F2 got even a little bit worse. Let's proceed to more sophisticated models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff06329",
   "metadata": {},
   "source": [
    "#### Ridge classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98415f18",
   "metadata": {},
   "source": [
    "Next, let's try ridge classifier. Ridge classifier also builds a regression model, but uses different loss function than logit. It includes penalty for too high parameter values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c628e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeClassifier\n",
    "\n",
    "model_ridge = RidgeClassifier(random_state = 12345)\n",
    "alpha = [0.01, 0.1, 0.25, 0.5, 0.75, 1.0, 5.0, 10.0]\n",
    "\n",
    "grid_ridge = dict(alpha = alpha)\n",
    "cv_5_3 = RepeatedKFold(n_splits = 5, n_repeats = 3, random_state = 12345)\n",
    "\n",
    "grid_search_ridge = GridSearchCV(estimator = model_ridge, param_grid = grid_ridge, n_jobs = -1, cv = cv_5_3, scoring = f2_scorer)\n",
    "grid_result_ridge = grid_search_ridge.fit(X_train, Y_train)\n",
    "print(\"Best: %f using %s\" % (grid_search_ridge.best_score_, grid_result_ridge.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4177a3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best: %f using %s\" % (grid_search_ridge.best_score_, grid_search_ridge.best_score_))\n",
    "\n",
    "means = grid_result_ridge.cv_results_['mean_test_score']\n",
    "stds = grid_result_ridge.cv_results_['std_test_score']\n",
    "params = grid_result_ridge.cv_results_['params']\n",
    "\n",
    "for mean, param in zip(means, params):\n",
    "    print(\"%f with: %r\" % (mean, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b4bf07",
   "metadata": {},
   "source": [
    "We got better F2 score than for logit, but the improvement was only minor. However, due to the nature of ridge classifier, we might observe better scores on the testing data as this model is less prone to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6005ee",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7955f338",
   "metadata": {},
   "source": [
    "Now let's try XGBoost. \n",
    "\n",
    "As the model took me araound an hour to be calculated, I saved it in a separate file and load its results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775771b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from xgboost import XGBClassifier\n",
    "\n",
    "# cv_5 = KFold(n_splits = 5, random_state = 12345, shuffle = True)\n",
    "\n",
    "# model_xgb = XGBClassifier()\n",
    "\n",
    "# n_estimators = [100, 500]\n",
    "# learning_rate = [0.001, 0.01, 0.1]\n",
    "# subsample = [0.8, 1.0]\n",
    "# max_depth = [3, 5, 7]\n",
    "# grid = dict(learning_rate = learning_rate, n_estimators = n_estimators, subsample = subsample, max_depth = max_depth)\n",
    "\n",
    "# grid_search = GridSearchCV(estimator=model_xgb, param_grid=grid, n_jobs=-1, cv=cv_5, scoring = f2_scorer , verbose = 1)\n",
    "# grid_result = grid_search.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc59610f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Let's save the file\n",
    "# joblib.dump(grid_result, 'xgb_model.sav')\n",
    "\n",
    "# Load the model results\n",
    "grid_result_xgb = joblib.load('xgb_model.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc6c762",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best: %f using %s\" % (grid_result_xgb.best_score_, grid_result_xgb.best_params_))\n",
    "\n",
    "means = grid_result_xgb.cv_results_['mean_test_score']\n",
    "stds = grid_result_xgb.cv_results_['std_test_score']\n",
    "params = grid_result_xgb.cv_results_['params']\n",
    "\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b9752e",
   "metadata": {},
   "source": [
    "We managed to achieve a significant improvement in our F2 score! The best model had it around 0.855. We could maybe achieve even better score with more complex parameter tuning, but unfortunately it is very time consuming. We might also have a problem of overfitting even with cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615cd70a",
   "metadata": {},
   "source": [
    "#### K nearest neighbours"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d23fe6",
   "metadata": {},
   "source": [
    "KNN classifier also took quite long to calculate and I am loading it from a separate file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a489a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# model_KNN = KNeighborsClassifier()\n",
    "# n_neighbors = range(1, 15, 2)\n",
    "\n",
    "# grid_KNN = dict(n_neighbors = n_neighbors)\n",
    "\n",
    "# cv_5_3 = RepeatedKFold(n_splits = 5, random_state = 12345, n_repeats = 3)\n",
    "\n",
    "# grid_search_knn = GridSearchCV(estimator = model_KNN, param_grid = grid_KNN, n_jobs = -1, cv = cv_5_3, scoring = f2_scorer)\n",
    "# grid_result_knn = grid_search_knn.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d784c98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's save the file\n",
    "# joblib.dump(grid_result_knn, 'knn_model.sav')\n",
    "\n",
    "# Loading the file\n",
    "grid_result_knn = joblib.load('knn_model.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d50c7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best: %f using %s\" % (grid_result_knn.best_score_, grid_result_knn.best_params_))\n",
    "\n",
    "means = grid_result_knn.cv_results_['mean_test_score']\n",
    "stds = grid_result_knn.cv_results_['std_test_score']\n",
    "params = grid_result_knn.cv_results_['params']\n",
    "\n",
    "for mean, param in zip(means, params):\n",
    "    print(\"%f with: %r\" % (mean, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3ff506",
   "metadata": {},
   "source": [
    "For KNN, we also managed to significantly improve our F2 score compared to the baseline logit model. We got the best score of 0.76. However, it still worse than our score for XGBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1266d3cf",
   "metadata": {},
   "source": [
    "#### Comparison of the models on the testing sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d38e939",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import fbeta_score, accuracy_score, roc_auc_score\n",
    "\n",
    "def f2_scorer_final(y_true, y_pred):\n",
    "    return fbeta_score(y_true, y_pred, beta = 2)\n",
    "\n",
    "models = [[logistic_fitted, 'Logit'],\n",
    "          [logistic_fe_fitted, 'Logit Feature Selection'],\n",
    "          [grid_result_ridge, 'Ridge'],\n",
    "          [grid_result_knn, 'KNN'],\n",
    "          [grid_result_xgb, 'XGBoost']]\n",
    "\n",
    "for model, model_name in models:\n",
    "    predicted_train = model.predict(X_train)\n",
    "    predicted_test = model.predict(X_test)\n",
    "    print(model_name, \n",
    "          '\\n',\n",
    "          'F2 score train:', \"{:.4f}\".format(f2_scorer_final(Y_train, predicted_train)),\n",
    "          'F2 score test:', \"{:.4f}\".format(f2_scorer_final(Y_test, predicted_test)),\n",
    "          '\\n',\n",
    "          'ROC AUC train:', \"{:.4f}\".format(roc_auc_score(Y_train, predicted_train)),\n",
    "          'ROC AUC test:', \"{:.4f}\".format(roc_auc_score(Y_test, predicted_test)),\n",
    "          '\\n',\n",
    "          'Accuracy train:', \"{:.4f}\".format(accuracy_score(Y_train, predicted_train)),\n",
    "          'Accuracy test:', \"{:.4f}\".format(accuracy_score(Y_test, predicted_test)),\n",
    "          '\\n\\n',\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a478aa",
   "metadata": {},
   "source": [
    "We see that all of the considered classification models overfitted by a huuuge amount. To investigate it further, I added also other metrics, such as ROC AUC and Accuracy. We see that actually they are very bad, no better than a random assigment of classes. Even ridge classifier, which, in theory, is effective for overfitting, performed very bad :(\n",
    "\n",
    "I tried to investigate this problem further, checking corectness etc. but unfortunately I have not yet managed to identify the cause of this behaviour other than maybe particular data characteristics. Generally, to avoid overfitting one might use regularization techniques (such as the afromentioned ridge), early stopping of the model after certain number of iterations or reducing complexity of the model, but I had no time to properly implement and explore those options."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891f0feb",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8005f7",
   "metadata": {},
   "source": [
    "### Further project development\n",
    "\n",
    "Given more time and resources, I would like to further develop my project in the following areas (in the order of priority):\n",
    "- Investigating why my classification models performed so poorly on the test data\n",
    "- More developed feature engineering including: PCA of weather variables, maybe creating a variable counting extreme weather events (e.g. exceeding two sigmas from the mean), maybe exploring aggregation by median instead of mean\n",
    "- Better assignment of weather stations to BTS stations. I would like to double check my present solution and try to limit the number of BTS stations with higher (more than 50 km) distances from their weather stations. Alternatively, I would like to explore assigning maybe the weighted mean of parameters from weather stations in the neighborhood as I signalized earlier. Also, addressing the problem of weather stations in very specific locations where the weather is much different e.g. in the mountains\n",
    "- Detecting outliers for the econometric and machine learning models as I unfortunately did not have time to do that\n",
    "- More visualization for weather variables on the map - I believe that this is a very clear and readable method of presenting the data and I given more time I would like to make more graphs similar to the one I did for temperature\n",
    "- Exploring different clustering algorithms like DBSCAN\n",
    "- Completing the third machine learning task that is building a model predicting the percentage of anomalies for a given location and month :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42991a2",
   "metadata": {},
   "source": [
    "### Applying conclusions drawn from the data in practice\n",
    "\n",
    "- In the process of data preprocessing, we identified several issues with the quality of the data from weather stations (missing values, incorrect values, duplicates etc.). Looking at those, we might try to implement some solutions to improve data reporting, for example better equipment, double checking entered values of parameters etc.\n",
    "\n",
    "- Looking at the BTS station locations, some of the BTS stations were quite far away from the weather stations which may cause problems with overseeing weather conditions in those locations.\n",
    "\n",
    "- We might want to be extra careful and do more checks for stations that are most prone to extreme weather conditions…\n",
    "\n",
    "- … but, on the other hand, as shown later, weather conditions do not impact the percentages of anomalies in BTS stations functioning. This may actually mean that we do not have to worry about monitoring the weather conditions:) Obviously, data models are just models and we might have some reasons not to trust the data (as it was artificially generated) and use our own reasoning. \n",
    "\n",
    "- Clustering the weather / BTS stations allow us to identify stations often undergoing similar conditions. Therefore, if for example one station in a particular cluster starts malfunctioning, we might want to be extra careful about the other ones. \n",
    "\n",
    "- Similarly for classification of BTS stations that undergo a high percentage of anomalies.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
